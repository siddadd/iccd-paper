@article{Lowe2004,
author = {Lowe, David G.},
doi = {10.1023/B:VISI.0000029664.99615.94},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Lowe\_Distinctive Image Features from Scale-Invariant Keypoints\_2004.pdf:pdf},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
month = nov,
number = {2},
pages = {91--110},
title = {{Distinctive Image Features from Scale-Invariant Keypoints}},
volume = {60},
year = {2004}
}

@article{Benoit2010,
author = {Benoit, A. and Caplier, A. and Durette, B. and Herault, J.},
doi = {10.1016/j.cviu.2010.01.011},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Benoit et al.\_Using Human Visual System modeling for bio-inspired low level image processing\_2010.pdf:pdf},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
month = jul,
number = {7},
pages = {758--773},
publisher = {Elsevier Inc.},
title = {{Using Human Visual System modeling for bio-inspired low level image processing}},
volume = {114},
year = {2010}
}

@article{Karam2011,
abstract = {In this paper, a selective perceptual-based (SELP) framework is presented to reduce the complexity of popular super-resolution (SR) algorithms while maintaining the desired quality of the enhanced images/video. A perceptual human visual system model is proposed to compute local contrast sensitivity thresholds. The obtained thresholds are used to select which pixels are super-resolved based on the perceived visibility of local edges. Processing only a set of perceptually significant pixels reduces significantly the computational complexity of SR algorithms without losing the achievable visual quality. The proposed SELP framework is integrated into a maximum-a posteriori-based SR algorithm as well as a fast two-stage fusion-restoration SR estimator. Simulation results show a significant reduction on average in computational complexity with comparable signal-to-noise ratio gains and visual quality.},
author = {Karam, Lina J and Sadaka, Nabil G and Ferzli, Rony and Ivanovski, Zoran a},
doi = {10.1109/TIP.2011.2159324},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Karam et al.\_An efficient selective perceptual-based super-resolution estimator.\_2011.pdf:pdf},
issn = {1941-0042},
journal = {IEEE transactions on image processing : a publication of the IEEE Signal Processing Society},
month = dec,
number = {12},
pages = {3470--82},
pmid = {21672677},
title = {{An efficient selective perceptual-based super-resolution estimator.}},
volume = {20},
year = {2011}
}

@article{Giachetti1998,
author = {Giachetti, a. and Campani, M. and Torre, V.},
doi = {10.1109/70.660838},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Giachetti, Campani, Torre\_The use of optical flow for road navigation\_1998.pdf:pdf},
issn = {1042296X},
journal = {IEEE Transactions on Robotics and Automation},
number = {1},
pages = {34--48},
title = {{The use of optical flow for road navigation}},
volume = {14},
year = {1998}
}

@article{Hou2011,
abstract = {We introduce a simple image descriptor referred to as the image signature. We show, within the theoretical framework of sparse signal mixing, that this quantity spatially approximates the foreground of an image. We experimentally investigate whether this approximate foreground overlaps with visually conspicuous image locations by developing a saliency algorithm based on the image signature. This saliency algorithm predicts human fixation points best among competitors on the Bruce and Tsotsos [1] benchmark dataset and does so in much shorter running time. In a related experiment, we demonstrate with a change blindness dataset that the distance between images induced by the image signature is closer to human perceptual distance than can be achieved using other saliency algorithms, pixel-wise or GIST [2] descriptor methods.},
author = {Hou, Xiaodi and Harel, Jonathan and Koch, Christof},
doi = {10.1109/TPAMI.2011.146},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Hou, Harel, Koch\_Image Signature Highlighting Sparse Salient Regions.\_2011.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
month = jul,
number = {1},
pages = {194--201},
pmid = {21788665},
title = {{Image Signature: Highlighting Sparse Salient Regions.}},
volume = {34},
year = {2011}
}

@article{Lang2012,
abstract = {This paper addresses the problem of detecting salient areas within natural images. We shall mainly study the problem under unsupervised setting, i.e., saliency detection without learning from labeled images. A solution of multitask sparsity pursuit is proposed to integrate multiple types of features for detecting saliency collaboratively. Given an image described by multiple features, its saliency map is inferred by seeking the consistently sparse elements from the joint decompositions of multiple-feature matrices into pairs of low-rank and sparse matrices. The inference process is formulated as a constrained nuclear norm and as an l(2, 1)-norm minimization problem, which is convex and can be solved efficiently with an augmented Lagrange multiplier method. Compared with previous methods, which usually make use of multiple features by combining the saliency maps obtained from individual features, the proposed method seamlessly integrates multiple features to produce jointly the saliency map with a single inference step and thus produces more accurate and reliable results. In addition to the unsupervised setting, the proposed method can be also generalized to incorporate the top-down priors obtained from supervised environment. Extensive experiments well validate its superiority over other state-of-the-art methods.},
author = {Lang, Congyan and Liu, Guangcan and Yu, Jian and Yan, Shuicheng},
doi = {10.1109/TIP.2011.2169274},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Saliency Detection by Multitask Sparsity Pursuit.pdf:pdf},
issn = {1941-0042},
journal = {IEEE Transactions on Image Processing},
month = mar,
number = {3},
pages = {1327--38},
pmid = {21947527},
title = {{Saliency Detection by Multitask Sparsity Pursuit}},
volume = {21},
year = {2012}
}

@article{Collins,
author = {Collins, R.T.},
doi = {10.1109/CVPR.2003.1211475},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Collins\_Mean-shift blob tracking through scale space\_Unknown.pdf:pdf},
isbn = {0-7695-1900-8},
journal = {2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings.},
pages = {II--234--40},
publisher = {IEEE Comput. Soc},
title = {{Mean-shift blob tracking through scale space}},
volume = {2}
}

@article{Judd2011,
abstract = {When an observer looks at an image, his eyes fixate on a few select points. Fixations from different observers are often consistent-observers tend to look at the same locations. We investigate how image resolution affects fixation locations and consistency across humans through an eye-tracking experiment. We showed 168 natural images and 25 pink noise images at different resolutions to 64 observers. Each image was shown at eight resolutions (height between 4 and 512 pixels) and upsampled to 860 × 1024 pixels for display. The total amount of visual information available ranged from 1/8 to 16 cycles per degree, respectively. We measure how well one observer's fixations predict another observer's fixations on the same image at different resolutions using the area under the receiver operating characteristic (ROC) curves as a metric. We found that: (1) Fixations from lower resolution images can predict fixations on higher resolution images. (2) Human fixations are biased toward the center for all resolutions and this bias is stronger at lower resolutions. (3) Human fixations become more consistent as resolution increases until around 16-64 pixels (1/2 to 2 cycles per degree) after which consistency remains relatively constant despite the spread of fixations away from the center. (4) Fixation consistency depends on image complexity.},
author = {Judd, Tilke and Durand, Fredo and Torralba, Antonio},
doi = {10.1167/11.4.14},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Judd Fixations on Low Res Images Poster.pdf:pdf},
issn = {1534-7362},
journal = {Journal of vision},
keywords = {Adolescent,Adult,Attention,Attention: physiology,Female,Fixation,Form Perception,Form Perception: physiology,Humans,Male,Middle Aged,Ocular,Ocular: physiology,Photic Stimulation,Photic Stimulation: methods,ROC Curve,Saccades,Saccades: physiology,Young Adult},
month = jan,
number = {4},
pages = {1--20},
pmid = {21518823},
title = {{Fixations on Low-Resolution Images.}},
volume = {11},
year = {2011}
}

@inproceedings{Delaluz2002,
author = {Delaluz, V. and Sivasubramaniam, A. and Kandemir, M. and Vijaykrishnan, N. and Irwin, M.J.},
booktitle = {Design Automation Conference},
doi = {10.1109/DAC.2002.1012714},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/DRAM\_dac02.pdf:pdf},
isbn = {1-58113-461-4},
keywords = {dram,energy management,operating systems,scheduler},
mendeley-tags = {dram},
pages = {697--702},
publisher = {Acm},
title = {{Scheduler-based DRAM energy management}},
year = {2002}
}

@article{Xu2010,
abstract = {Visual saliency is the perceptual quality that makes some items in visual scenes stand out from their immediate contexts. Visual saliency plays important roles in natural vision in that saliency can direct eye movements, deploy attention, and facilitate tasks like object detection and scene understanding. A central unsolved issue is: What features should be encoded in the early visual cortex for detecting salient features in natural scenes? To explore this important issue, we propose a hypothesis that visual saliency is based on efficient encoding of the probability distributions (PDs) of visual variables in specific contexts in natural scenes, referred to as context-mediated PDs in natural scenes. In this concept, computational units in the model of the early visual system do not act as feature detectors but rather as estimators of the context-mediated PDs of a full range of visual variables in natural scenes, which directly give rise to a measure of visual saliency of any input stimulus. To test this hypothesis, we developed a model of the context-mediated PDs in natural scenes using a modified algorithm for independent component analysis (ICA) and derived a measure of visual saliency based on these PDs estimated from a set of natural scenes. We demonstrated that visual saliency based on the context-mediated PDs in natural scenes effectively predicts human gaze in free-viewing of both static and dynamic natural scenes. This study suggests that the computation based on the context-mediated PDs of visual variables in natural scenes may underlie the neural mechanism in the early visual cortex for detecting salient features in natural scenes.},
author = {Xu, Jinhua and Yang, Zhiyong and Tsien, Joe Z},
doi = {10.1371/journal.pone.0015796},
file = {:Users/siddharthadvani/Documents/MDL/ARL/Papers/Emergence of Visual Saliency from Natural Scenes via Context-Mediated Probability Distributions Coding.pdf:pdf},
issn = {1932-6203},
journal = {PloS one},
keywords = {Adult,Algorithms,Biological,Brain Mapping,Brain Mapping: methods,Calibration,Humans,Models,Neurons,Neurons: metabolism,Ocular,Ocular: physiology,Principal Component Analysis,Probability,ROC Curve,Saliency,Statistical,Video Recording,Vision,Visual Perception},
mendeley-tags = {Saliency},
month = jan,
number = {12},
pages = {e15796},
pmid = {21209963},
title = {{Emergence of visual saliency from natural scenes via context-mediated probability distributions coding.}},
volume = {5},
year = {2010}
}

@inproceedings{Mishra2009,
author = {Mishra, Ajay and Aloimonos, Yiannis},
booktitle = {2009 IEEE 12th International Conference on Computer Vision},
doi = {10.1109/ICCV.2009.5459254},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/iccv2009\_activeSeg.pdf:pdf},
isbn = {978-1-4244-4420-5},
keywords = {fixation},
mendeley-tags = {fixation},
month = sep,
pages = {468--475},
publisher = {Ieee},
title = {{Active segmentation with fixation}},
year = {2009}
}

@article{Wolfe2004,
author = {Wolfe, Jeremy M and Horowitz, Todd S},
doi = {10.1038/nrn1411},
file = {:Users/siddharthadvani/Documents/MDL/ARL/Papers/WhatAttributesGuidetheDeploymentofSaliency.pdf:pdf},
issn = {1471-003X},
journal = {Nature Reviews Neuroscience},
keywords = {Animals,Attention,Attention: physiology,Brain,Brain: physiology,Cues,Humans,Pattern Recognition,Psychomotor Performance,Psychomotor Performance: physiology,Saliency,Visual,Visual Perception,Visual Perception: physiology,Visual: physiology},
mendeley-tags = {Saliency},
month = jun,
number = {6},
pages = {495--501},
pmid = {15152199},
title = {{What attributes guide the deployment of visual attention and how do they do it?}},
volume = {5},
year = {2004}
}

@article{Daugman1985,
author = {Daugman, John G},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Daugman Uncertainty.pdf:pdf},
journal = {Journal of the Optical Society of America},
number = {7},
pages = {1160--1169},
title = {{Uncertainty Relation for Resolution in Space, Spatial Frequency, and Orientation Optimized by Two-Dimensional Visual Cortical Filters}},
volume = {2},
year = {1985}
}

@inproceedings{Park2012,
author = {Park, Sungho and Cheol, Yong and Cho, Peter and Irick, Kevin M and Narayanan, Vijaykrishnan and Overview, A},
booktitle = {IEEE Asia and South Pacific - Design Automation Conference},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/A reconfigurable platform for the design and verification of domain-specific accelerators.pdf:pdf},
isbn = {9781467307727},
number = {Mdl},
pages = {108--113},
title = {{A Reconfigurable Platform for the Design and Verification of Domain-Specific Accelerators}},
year = {2012}
}

@article{Burgsteiner2006,
author = {Burgsteiner, Harald and Kr\"{o}ll, Mark and Leopold, Alexander and Steinbauer, Gerald},
doi = {10.1007/s10489-006-0007-1},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Burgsteiner et al.\_Movement prediction from real-world images using a liquid state machine\_2006.pdf:pdf},
issn = {0924-669X},
journal = {Applied Intelligence},
keywords = {liquid state machine,recurrent spiking neural networks,robotics,time series prediction},
month = nov,
number = {2},
pages = {99--109},
title = {{Movement prediction from real-world images using a liquid state machine}},
volume = {26},
year = {2006}
}

@inproceedings{Kestur2011,
author = {Kestur, Srinidhi and Dantara, Dharav and Narayanan, Vijaykrishnan},
booktitle = {Design, Automation, and Test in Europe},
file = {:Users/siddharthadvani/Documents/MDL/ARL/Papers/MDL\_SHARC- A streaming model for FPGA accelerators and its application to Saliency.pdf:pdf},
isbn = {9783981080179},
keywords = {FPGA,Saliency},
mendeley-tags = {FPGA,Saliency},
title = {{SHARC : A Streaming Model for FPGA Accelerators and its Application to Saliency}},
year = {2011}
}

@inproceedings{Liu2011,
author = {Liu, Song and {Pattabiraman Karthik} and Moscibroda, Thomas and Zorn, Benjamin G},
booktitle = {Architectural Support for Programming Languages and Operating Systems},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Flikker.pdf:pdf},
isbn = {9781450302661},
keywords = {critical,dram refresh,power-savings,soft errors},
title = {{Flikker : Saving DRAM Refresh-power through Critical Data Partitioning}},
year = {2011}
}

@article{Rosenfeld2011,
author = {Rosenfeld, P and Cooper-Balis, E and Jacob, B},
doi = {10.1109/L-CA.2011.4},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/DRAMSim2.pdf:pdf},
issn = {1556-6056},
journal = {IEEE Computer Architecture Letters},
month = jan,
number = {1},
pages = {16--19},
title = {{DRAMSim2: A Cycle Accurate Memory System Simulator}},
volume = {10},
year = {2011}
}

@article{Tatler2011,
author = {Tatler, Benjamin W and Hayhoe, Mary M and Land, Michael F and Ballard, Dana H},
doi = {10.1167/11.5.5.Introduction},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Tatler et al.\_Eye guidance in natural vision Reinterpreting salience\_2011.pdf:pdf},
journal = {Journal of Vision},
keywords = {1,10,11,1167,2011,23,5,Saliency,b,ballard,citation,content,d,doi,eye guidance in natural,eye movements,f,h,hayhoe,http,journal of vision,journalofvision,land,learning,m,natural tasks,org,prediction,reinterpreting,reward,salience,tatler,vision,w,www},
mendeley-tags = {Saliency},
pages = {1--23},
title = {{Eye Guidance in Natural Vision : Reinterpreting Salience}},
volume = {11},
year = {2011}
}

@article{Kuchnio2011,
author = {Kuchnio, Peter and Capson, David},
doi = {10.1109/CRV.2011.22},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Kuchnio, Capson\_GPU-Accelerated Foveation for Video Frame Rate Tracking\_2011.pdf:pdf},
isbn = {978-1-61284-430-5},
journal = {2011 Canadian Conference on Computer and Robot Vision},
keywords = {-gpu,algorithm in cuda are,cuda,details of,discussed in section iii,foveation,image-based visual servo,mapping of a foveated,motion segmentation,optical flow,scription of the parallel},
month = may,
pages = {117--124},
publisher = {Ieee},
title = {{GPU-Accelerated Foveation for Video Frame Rate Tracking}},
year = {2011}
}

@book{Land2009,
author = {Land, Michael F and Tatler, Benjamin W},
booktitle = {Psychology Press},
keywords = {Micheal F. Land and Benjamin W. Tatler,Vision},
mendeley-tags = {Vision},
publisher = {Oxford},
title = {{Looking and Acting: Vision and eye movements in natural behaviour}},
year = {2009}
}

@inproceedings{Farhadi2010,
author = {Farhadi, Ali and Hejrati, Mohsen and Sadeghi, Mohammad Amin and Young, Peter and Rashtchian, Cyrus and Hockenmaier, Julia and Forsyth, David},
booktitle = {European Conference on Computer Vision},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/sentence.pdf:pdf},
keywords = {scene},
mendeley-tags = {scene},
title = {{Every Picture Tells a Story : Generating Sentences from Images}},
year = {2010}
}

@article{Wal1992,
author = {Wal, Gooitzen S. and Burt, Peter J.},
doi = {10.1007/BF00055150},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Wal, Burt\_A VLSI pyramid chip for multiresolution image analysis\_1992.pdf:pdf},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
month = sep,
number = {3},
pages = {177--189},
title = {{A VLSI pyramid chip for multiresolution image analysis}},
volume = {8},
year = {1992}
}

@article{Judd2009,
author = {Judd, Tilke and Ehinger, Krista and Durand, Fredo and Torralba, Antonio},
doi = {10.1109/ICCV.2009.5459462},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Judd Learning to Predict Where Humans Look Poster.pdf:pdf},
isbn = {978-1-4244-4420-5},
journal = {2009 IEEE 12th International Conference on Computer Vision},
month = sep,
pages = {2106--2113},
publisher = {Ieee},
title = {{Learning to Predict Where Humans Look}},
year = {2009}
}

@techreport{Mutch2010,
address = {Cambridge, MA},
author = {Mutch, Jim and Knoblich, Ulf and Poggio, Tomaso},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/MIT-CSAIL-TR-2010-013.pdf:pdf},
institution = {Massachusetts Institute of Technology},
keywords = {GPU},
mendeley-tags = {GPU},
title = {{CNS : a GPU-based framework for simulating cortically-organized networks}},
year = {2010}
}

@inproceedings{Judd2009a,
author = {Judd, Tilke and Ehinger, Krista and Torralba, Antonio},
booktitle = {2009 IEEE 12th International Conference on Computer Vision},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Juddwherepeoplelook.pdf:pdf},
keywords = {Saliency},
mendeley-tags = {Saliency},
pages = {2106--2113},
title = {{Learning to Predict Where Humans Look}},
year = {2009}
}

@article{Mutch2008,
author = {Mutch, Jim and Lowe, David G.},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/ijcv2008\_mutch\_lowe.pdf:pdf},
journal = {International Journal of Computer Vision},
keywords = {object},
mendeley-tags = {object},
title = {{Object class recognition and localization using sparse features with limited receptive fields}},
year = {2008}
}

@article{Jiang2012,
author = {Jiang, Zhuolin and Lin, Zhe and Davis, Larry S.},
doi = {10.1016/j.cviu.2012.02.004},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Jiang, Lin, Davis\_Class consistent k-means Application to face and action recognition\_2012.pdf:pdf},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
month = jun,
number = {6},
pages = {730--741},
publisher = {Elsevier Inc.},
title = {{Class consistent k-means: Application to face and action recognition}},
volume = {116},
year = {2012}
}

@article{Wolfe2002,
author = {Wolfe, Jeremy M},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Wolfe\_Visual Search\_2002.pdf:pdf},
pages = {1--41},
title = {{Visual Search}},
year = {2002}
}

@inproceedings{Liu2012,
author = {Liu, Jamie},
booktitle = {International Symposium on Computer Architecture},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/RAIDR.pdf:pdf},
isbn = {9781467304764},
keywords = {dram},
mendeley-tags = {dram},
number = {c},
pages = {1--12},
title = {{RAIDR : Retention-Aware Intelligent DRAM Refresh}},
volume = {00},
year = {2012}
}

@article{Parkhurst2002,
abstract = {A biologically motivated computational model of bottom-up visual selective attention was used to examine the degree to which stimulus salience guides the allocation of attention. Human eye movements were recorded while participants viewed a series of digitized images of complex natural and artificial scenes. Stimulus dependence of attention, as measured by the correlation between computed stimulus salience and fixation locations, was found to be significantly greater than that expected by chance alone and furthermore was greatest for eye movements that immediately follow stimulus onset. The ability to guide attention of three modeled stimulus features (color, intensity and orientation) was examined and found to vary with image type. Additionally, the effect of the drop in visual sensitivity as a function of eccentricity on stimulus salience was examined, modeled, and shown to be an important determiner of attentional allocation. Overall, the results indicate that stimulus-driven, bottom-up mechanisms contribute significantly to attentional guidance under natural viewing conditions.},
author = {Parkhurst, Derrick and Law, Klinton and Niebur, Ernst},
file = {:Users/siddharthadvani/Documents/MDL/ARL/Papers/Modeling the role of salience in the allocation of overt visual attention.pdf:pdf},
issn = {0042-6989},
journal = {Vision Research},
keywords = {Analysis of Variance,Attention,Attention: physiology,Biological,Computer Simulation,Eye Movements,Eye Movements: physiology,Female,Humans,Male,Models,Normal Distribution,Visual Perception,Visual Perception: physiology},
month = jan,
number = {1},
pages = {107--23},
pmid = {11804636},
title = {{Modeling the Role of Salience in the Allocation of Overt Visual Attention}},
volume = {42},
year = {2002}
}

@article{Bandera1989,
author = {Bandera, C. and Scott, P.D.},
doi = {10.1109/ICSMC.1989.71367},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Bandera, Scott\_Foveal machine vision systems\_1989.pdf:pdf},
journal = {Conference Proceedings., IEEE International Conference on Systems, Man and Cybernetics},
keywords = {Foveation},
mendeley-tags = {Foveation},
pages = {596--599},
publisher = {Ieee},
title = {{Foveal machine vision systems}},
year = {1989}
}

@article{Geisler1998,
author = {Geisler, Wilson S and Perry, Jeffrey S},
file = {:Users/siddharthadvani/Documents/MDL/ARL/Papers/ARealTimeFoveatedMRSystemforLowBWVideoCommunication.pdf:pdf},
journal = {Proceedings of the SPIE: The International Society for Optical Engineering},
keywords = {Foveation,eye tracking,foveated imaging,foveation,human,motion compensation,multiresolution pyramid,video,video compression,vision,zero-tree coding},
mendeley-tags = {Foveation},
title = {{A Real-Time Foveated Multiresolution System for Low-Bandwidth Video Communication}},
volume = {3299},
year = {1998}
}

@article{Deng2009,
author = {Deng, L. and Chakrabarti, C. and Pitsianis, N. and Sun, X.},
doi = {10.1117/12.834184},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Deng et al.\_Automated optimization of look-up table implementation for function evaluation on FPGAs\_2009.pdf:pdf},
journal = {Proceedings of SPIE},
keywords = {automatic generation,fpgas,function evaluation,look-up table,resource minimization},
pages = {744413--744413--9},
publisher = {Spie},
title = {{Automated optimization of look-up table implementation for function evaluation on FPGAs}},
year = {2009}
}

@article{Young1998,
abstract = {This paper presents a method for detecting and classifying a target from its foveal (graded resolution) imagery using a multiresolution neural network. Target identification decisions are based on minimizing an energy function. This energy function is evaluated by comparing a candidate blob with a library of target models at several levels of resolution simultaneously available in the current foveal image. For this purpose, a concurrent (top-down-and-bottom-up) matching procedure is implemented via a novel multilayer Hopfield neural network. The associated energy function supports not only interactions between cells at the same resolution level, but also between sets of nodes at distinct resolution levels. This permits features at different resolution levels to corroborate or refute one another contributing to an efficient evaluation of potential matches. Gaze control, refoveation to more salient regions of the available image space, is implemented as a search for high resolution features which will disambiguate the candidate blob. Tests using real two-dimensional (2-D) objects and their simulated foveal imagery are provided.},
author = {Young, S S and Scott, P D and Bandera, C},
doi = {10.1109/83.704306},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Young, Scott, Bandera\_Foveal automatic target recognition using a multiresolution neural network.\_1998.pdf:pdf},
issn = {1057-7149},
journal = {IEEE transactions on image processing : a publication of the IEEE Signal Processing Society},
month = jan,
number = {8},
pages = {1122--35},
pmid = {18276329},
title = {{Foveal automatic target recognition using a multiresolution neural network.}},
volume = {7},
year = {1998}
}

@article{Hrabar2005,
author = {Hrabar, S. and Sukhatme, G.S. and Corke, P. and Usher, K. and Roberts, J.},
doi = {10.1109/IROS.2005.1544998},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Hrabar et al.\_Combined optic-flow and stereo-based navigation of urban canyons for a UAV\_2005.pdf:pdf},
isbn = {0-7803-8912-3},
journal = {2005 IEEE/RSJ International Conference on Intelligent Robots and Systems},
pages = {3309--3316},
publisher = {Ieee},
title = {{Combined optic-flow and stereo-based navigation of urban canyons for a UAV}},
year = {2005}
}

@article{Riche2012,
author = {Riche, Nicolas and Mancas, Matei and Culibrk, Dubravko and Crnojevic, Vladimir and Gosselin, Bernard and Dutoit, Thierry},
file = {:Users/siddharthadvani/Documents/MDL/ARL/Papers/Dynamic saliency models and human attention- a comparative study on videos.pdf:pdf},
journal = {Proceedings of the 11th Asian Conference on Computer Vision (ACCV)},
keywords = {Saliency},
mendeley-tags = {Saliency},
title = {{Dynamic saliency models and human attention : a comparative study on videos}},
year = {2012}
}

@article{Itti2001,
abstract = {Five important trends have emerged from recent work on computational models of focal visual attention that emphasize the bottom-up, image-based control of attentional deployment. First, the perceptual saliency of stimuli critically depends on the surrounding context. Second, a unique 'saliency map' that topographically encodes for stimulus conspicuity over the visual scene has proved to be an efficient and plausible bottom-up control strategy. Third, inhibition of return, the process by which the currently attended location is prevented from being attended again, is a crucial element of attentional deployment. Fourth, attention and eye movements tightly interplay, posing computational challenges with respect to the coordinate system used to control attention. And last, scene understanding and object recognition strongly constrain the selection of attended locations. Insights from these five key areas provide a framework for a computational and neurobiological understanding of visual attention.},
author = {Itti, L and Koch, C},
doi = {10.1038/35058500},
file = {:Users/siddharthadvani/Documents/MDL/ARL/Papers/Itti\_Koch\_ComputationalModellingVisualAttention.pdf:pdf},
issn = {1471-003X},
journal = {Nature Reviews Neuroscience},
keywords = {Animals,Attention,Attention: physiology,Computer Simulation,Humans,Models,Neurological,Neurons,Neurons: metabolism,Visual Cortex,Visual Cortex: physiology,Visual Perception,Visual Perception: physiology},
month = mar,
number = {3},
pages = {194--203},
pmid = {11256080},
title = {{Computational Modelling of Visual Attention}},
volume = {2},
year = {2001}
}

@article{Thevenaz2000,
abstract = {Based on the theory of approximation, this paper presents a unified analysis of interpolation and resampling techniques. An important issue is the choice of adequate basis functions. We show that, contrary to the common belief, those that perform best are not interpolating. By opposition to traditional interpolation, we call their use generalized interpolation; they involve a prefiltering step when correctly applied. We explain why the approximation order inherent in any basis function is important to limit interpolation artifacts. The decomposition theorem states that any basis function endowed with approximation order can be expressed as the convolution of a B-spline of the same order with another function that has none. This motivates the use of splines and spline-based functions as a tunable way to keep artifacts in check without any significant cost penalty. We discuss implementation and performance issues, and we provide experimental evidence to support our claims.},
author = {Th\'{e}venaz, P and Blu, T and Unser, M},
doi = {10.1109/42.875199},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Th\'{e}venaz, Blu, Unser\_Interpolation revisited.\_2000.pdf:pdf},
issn = {0278-0062},
journal = {IEEE transactions on medical imaging},
keywords = {Artifacts,Costs and Cost Analysis,Diagnostic Imaging,Diagnostic Imaging: economics,Diagnostic Imaging: methods,Fourier Analysis,Humans,Image Processing, Computer-Assisted,Image Processing, Computer-Assisted: methods,Mathematics},
month = jul,
number = {7},
pages = {739--58},
pmid = {11055789},
title = {{Interpolation revisited.}},
volume = {19},
year = {2000}
}

@article{Bolduc1998,
author = {Bolduc, Marc and Levine, Martin D.},
doi = {10.1006/cviu.1997.0560},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Bolduc, Levine\_A Review of Biologically Motivated Space-Variant Data Reduction Models for Robotic Vision\_1998.pdf:pdf},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
month = feb,
number = {2},
pages = {170--184},
title = {{A Review of Biologically Motivated Space-Variant Data Reduction Models for Robotic Vision}},
volume = {69},
year = {1998}
}

@article{Vijayakumar2001,
author = {Vijayakumar, S. and Conradt, J. and Shibata, T. and Schaal, S.},
doi = {10.1109/IROS.2001.976418},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Vijayakumar et al.\_Overt visual attention for a humanoid robot\_2001.pdf:pdf},
isbn = {0-7803-6612-3},
journal = {Proceedings 2001 IEEE/RSJ International Conference on Intelligent Robots and Systems. Expanding the Societal Role of Robotics in the the Next Millennium (Cat. No.01CH37180)},
number = {Iros},
pages = {2332--2337},
publisher = {Ieee},
title = {{Overt visual attention for a humanoid robot}},
volume = {4},
year = {2001}
}

@article{Land2000,
abstract = {In cricket, a batsman watches a fast bowler's ball come toward him at a high and unpredictable speed, bouncing off ground of uncertain hardness. Although he views the trajectory for little more than half a second, he can accurately judge where and when the ball will reach him. Batsmen's eye movements monitor the moment when the ball is released, make a predictive saccade to the place where they expect it to hit the ground, wait for it to bounce, and follow its trajectory for 100-200 ms after the bounce. We show how information provided by these fixations may allow precise prediction of the ball's timing and placement. Comparing players with different skill levels, we found that a short latency for the first saccade distinguished good from poor batsmen, and that a cricket player's eye movement strategy contributes to his skill in the game.},
author = {Land, M F and McLeod, P},
doi = {10.1038/81887},
file = {:Users/siddharthadvani/Documents/MDL/ARL/Papers/From eye movements to actions- how batsmen hit the ball.pdf:pdf},
issn = {1097-6256},
journal = {Nature Neuroscience},
keywords = {Adult,Baseball,Baseball: physiology,Head Movements,Head Movements: physiology,Humans,Male,Motion Perception,Motion Perception: physiology,Motor Skills,Motor Skills: physiology,Psychomotor Performance,Psychomotor Performance: physiology,Pursuit,Reaction Time,Reaction Time: physiology,Saccades,Saccades: physiology,Smooth,Smooth: physiology,Sports,Sports: physiology,Vision},
mendeley-tags = {Vision},
month = dec,
number = {12},
pages = {1340--5},
pmid = {11100157},
title = {{From eye movements to actions: how batsmen hit the ball.}},
volume = {3},
year = {2000}
}

@article{Lehmann1999,
abstract = {Image interpolation techniques often are required in medical imaging for image generation (e.g., discrete back projection for inverse Radon transform) and processing such as compression or resampling. Since the ideal interpolation function spatially is unlimited, several interpolation kernels of finite size have been introduced. This paper compares 1) truncated and windowed sinc; 2) nearest neighbor; 3) linear; 4) quadratic; 5) cubic B-spline; 6) cubic; g) Lagrange; and 7) Gaussian interpolation and approximation techniques with kernel sizes from 1 x 1 up to 8 x 8. The comparison is done by: 1) spatial and Fourier analyses; 2) computational complexity as well as runtime evaluations; and 3) qualitative and quantitative interpolation error determinations for particular interpolation tasks which were taken from common situations in medical image processing. For local and Fourier analyses, a standardized notation is introduced and fundamental properties of interpolators are derived. Successful methods should be direct current (DC)-constant and interpolators rather than DC-inconstant or approximators. Each method's parameters are tuned with respect to those properties. This results in three novel kernels, which are introduced in this paper and proven to be within the best choices for medical image interpolation: the 6 x 6 Blackman-Harris windowed sinc interpolator, and the C2-continuous cubic kernels with N = 6 and N = 8 supporting points. For quantitative error evaluations, a set of 50 direct digital X rays was used. They have been selected arbitrarily from clinical routine. In general, large kernel sizes were found to be superior to small interpolation masks. Except for truncated sinc interpolators, all kernels with N = 6 or larger sizes perform significantly better than N = 2 or N = 3 point methods (p < 0.005). However, the differences within the group of large-sized kernels were not significant. Summarizing the results, the cubic 6 x 6 interpolator with continuous second derivatives, as defined in (24), can be recommended for most common interpolation tasks. It appears to be the fastest six-point kernel to implement computationally. It provides eminent local and Fourier properties, is easy to implement, and has only small errors. The same characteristics apply to B-spline interpolation, but the 6 x 6 cubic avoids the intrinsic border effects produced by the B-spline technique. However, the goal of this study was not to determine an overall best method, but to present a comprehensive catalogue of methods in a uniform terminology, to define general properties and requirements of local techniques, and to enable the reader to select that method which is optimal for his specific application in medical imaging.},
author = {Lehmann, T M and G\"{o}nner, C and Spitzer, K},
doi = {10.1109/42.816070},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Lehmann, G\"{o}nner, Spitzer\_Survey interpolation methods in medical image processing.\_1999.pdf:pdf},
issn = {0278-0062},
journal = {IEEE transactions on medical imaging},
keywords = {Diagnostic Imaging,Fourier Analysis,Humans,Image Processing, Computer-Assisted,Image Processing, Computer-Assisted: methods},
month = nov,
number = {11},
pages = {1049--75},
pmid = {10661324},
title = {{Survey: interpolation methods in medical image processing.}},
volume = {18},
year = {1999}
}

@article{Einhauser2003,
author = {Einhauser, Wolfgang and Konig, Peter},
doi = {10.1046/j.1460-9568.2003.02508.x},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Einhauser, Konig\_Does luminance-contrast contribute to a saliency map for overt visual attention\_2003.pdf:pdf},
issn = {0953-816X},
journal = {European Journal of Neuroscience},
keywords = {eye movements,hierarchy,human,top-down,visual system},
month = mar,
number = {5},
pages = {1089--1097},
title = {{Does luminance-contrast contribute to a saliency map for overt visual attention?}},
volume = {17},
year = {2003}
}

@inproceedings{Liu2013,
author = {Liu, Jamie and Jaiyen, Ben and Kim, Yoongu and Wilkerson, Chris},
booktitle = {International Symposium on Computer Architecture},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/dram-retention\_isca13.pdf:pdf},
isbn = {9781450320795},
title = {{An Experimental Study of Data Retention Behavior in Modern DRAM Devices : Implications for Retention Time Profiling Mechanisms}},
year = {2013}
}

@article{Karam2012,
author = {Karam, Samuel F . Dodge and Lina J .},
file = {:Users/siddharthadvani/Documents/MDL/ARL/Papers/gesture\_ICIP2012\_cameraready\_final.pdf:pdf},
journal = {International Conference on Image Processing},
title = {{Attentive gesture recognition}},
year = {2012}
}

@article{DeBole2011,
author = {DeBole, M. and Maashri, a. Al and Cotter, M. and Yu, C-L. and Chakrabarti, C. and Narayanan, V.},
doi = {10.1109/ICCAD.2011.6105351},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/DeBole et al.\_A framework for accelerating neuromorphic-vision algorithms on FPGAs\_2011.pdf:pdf},
isbn = {978-1-4577-1400-9},
journal = {2011 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)},
keywords = {- multi-fpga partitioning,fpga application mapping,fpga programming,neuromorphic vision algorithms},
month = nov,
pages = {810--813},
publisher = {Ieee},
title = {{A framework for accelerating neuromorphic-vision algorithms on FPGAs}},
year = {2011}
}

@article{Jablin2009,
author = {Jablin, Thomas and Upton, Dan and August, David and Hazelwood, Kim and Mahlke, Scott},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/multicore compilation strategies and challenges.pdf:pdf},
journal = {IEEE Signal Processing Magazine},
keywords = {Compiler},
mendeley-tags = {Compiler},
number = {November},
pages = {55--63},
title = {{Multicore Compilation Strategies and Challenges [}},
year = {2009}
}

@article{Bruce2006,
author = {Bruce, Neil D B and Tsotsos, John K},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Bruce\_Saliency Based on Information Maximization.pdf\_Unknown.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
pages = {155--162},
title = {{Saliency Based on Information Maximization}},
volume = {18},
year = {2006}
}

@techreport{Judd2012,
address = {Cambridge, MA},
author = {Judd, Tilke and Durand, Fr\o do and Torralba, Antonio},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Judd, Durand, Torralba\_A Benchmark of Computational Models of Saliency to Predict Human Fixations A Benchmark of Computational Models of Saliency to Predict Human Fixations\_2012.pdf:pdf},
institution = {Massachusetts Institute of Technology},
keywords = {Saliency},
mendeley-tags = {Saliency},
title = {{A Benchmark of Computational Models of Saliency to Predict Human Fixations}},
year = {2012}
}

@inproceedings{Nere2011,
author = {Nere, Andrew and Hashmi, Atif and Lipasti, Mikko},
booktitle = {2011 IEEE International Parallel \& Distributed Processing Symposium},
doi = {10.1109/IPDPS.2011.88},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/nere\_ipdps\_2011.pdf:pdf},
isbn = {978-1-61284-372-8},
keywords = {-cortical learning algorithms,gpgpu,profiling},
month = may,
pages = {906--920},
publisher = {Ieee},
title = {{Profiling Heterogeneous Multi-GPU Systems to Accelerate Cortically Inspired Learning Algorithms}},
year = {2011}
}

@article{Grzyb2009,
author = {Grzyb, Beata J. and Chinellato, Eris and Wojcik, Grzegorz M. and Kaminski, Wieslaw a.},
doi = {10.1109/IJCNN.2009.5179025},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Grzyb et al.\_Facial expression recognition based on Liquid State Machines built of alternative neuron models\_2009.pdf:pdf},
isbn = {978-1-4244-3548-7},
journal = {2009 International Joint Conference on Neural Networks},
month = jun,
number = {1},
pages = {1011--1017},
publisher = {Ieee},
title = {{Facial expression recognition based on Liquid State Machines built of alternative neuron models}},
year = {2009}
}

@article{Burt1983,
author = {Burt, Peter J. and Adelson, Edward H.},
doi = {10.1145/245.247},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Burt, Adelson\_A multiresolution spline with application to image mosaics\_1983.pdf:pdf},
issn = {07300301},
journal = {ACM Transactions on Graphics},
month = oct,
number = {4},
pages = {217--236},
title = {{A multiresolution spline with application to image mosaics}},
volume = {2},
year = {1983}
}

@article{Verstraeten2005,
author = {Verstraeten, D. and Schrauwen, B. and Stroobandt, D. and {Van Campenhout}, J.},
doi = {10.1016/j.ipl.2005.05.019},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Verstraeten et al.\_Isolated word recognition with the Liquid State Machine a case study\_2005.pdf:pdf},
issn = {00200190},
journal = {Information Processing Letters},
keywords = {liquid state machine,parallel processing,speech recognition,spiking neural networks},
month = sep,
number = {6},
pages = {521--528},
title = {{Isolated word recognition with the Liquid State Machine: a case study}},
volume = {95},
year = {2005}
}

@article{Chen2006,
author = {Chen, G. and Xue, L. and Kim, J. and Sobti, K. and Deng, L. and Sun, X. and Pitsianis, N. and Chakrabarti, C. and Kandemir, M. and Vijaykrishnan, N.},
doi = {10.1109/SOCC.2006.283861},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Chen et al.\_Geometric Tiling for Reducing Power Consumption in Structured Matrix Operations\_2006.pdf:pdf},
isbn = {0-7803-9782-7},
journal = {2006 IEEE International SOC Conference},
month = sep,
pages = {113--114},
publisher = {Ieee},
title = {{Geometric Tiling for Reducing Power Consumption in Structured Matrix Operations}},
year = {2006}
}

@inproceedings{Venkatesh2011,
author = {Venkatesh, Ganesh and Sampson, Jack and Goulding-hotta, Nathan and Venkata, Sravanthi Kota and Taylor, Michael Bedford and Swanson, Steven},
booktitle = {IEEE Micro},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Micro2011QASICS.pdf:pdf},
isbn = {9781450310536},
keywords = {Dark Silicon,conservation core,dark silicon,heterogeneous many-core,merging,q s c ore,specialization,utilization,wall},
mendeley-tags = {Dark Silicon},
title = {{QSCORES : Trading Dark Silicon for Scalable Energy Efficiency with Quasi-Specific Cores}},
year = {2011}
}

@article{Walther2006,
abstract = {Selective visual attention is believed to be responsible for serializing visual information for recognizing one object at a time in a complex scene. But how can we attend to objects before they are recognized? In coherence theory of visual cognition, so-called proto-objects form volatile units of visual information that can be accessed by selective attention and subsequently validated as actual objects. We propose a biologically plausible model of forming and attending to proto-objects in natural scenes. We demonstrate that the suggested model can enable a model of object recognition in cortex to expand from recognizing individual objects in isolation to sequentially recognizing all objects in a more complex scene.},
author = {Walther, Dirk and Koch, Christof},
doi = {10.1016/j.neunet.2006.10.001},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Walther, Koch\_Modeling attention to salient proto-objects.\_2006.pdf:pdf},
issn = {0893-6080},
journal = {Neural Networks},
keywords = {Attention,Biological,Computer Simulation,Discrimination Learning,Discrimination Learning: physiology,Feedback,Humans,Models,Neural Networks (Computer),Pattern Recognition,Photic Stimulation,Photic Stimulation: methods,ROC Curve,Visual,Visual: physiology},
month = nov,
number = {9},
pages = {1395--407},
pmid = {17098563},
title = {{Modeling Attention to Salient Proto-objects}},
volume = {19},
year = {2006}
}

@article{Vo2008,
author = {V\~{o}, Melissa L and Schneider, Werner X and Matthias, Ellen},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/vo\_2008\_jemr.pdf:pdf},
journal = {Journal of Eye Movement Research},
keywords = {Vision,processing efficiency,scene perception,theory of visual attention,transsaccadic memory,tva},
mendeley-tags = {Vision},
number = {2},
pages = {1--13},
title = {{Transsaccadic Scene Memory Revisited : A ' Theory of Visual Attention ( TVA )' Based Approach to Recognition Memory and Confidence for Objects in Naturalistic Scenes .}},
volume = {2},
year = {2008}
}

@article{Hu2014,
abstract = {About ten years ago, HMAX was proposed as a simple and biologically feasible model for object recognition, based on how the visual cortex processes information. However, the model does not encompass sparse firing, which is a hallmark of neurons at all stages of the visual pathway. The current paper presents an improved model, called sparse HMAX, which integrates sparse firing. This model is able to learn higher-level features of objects on unlabeled training images. Unlike most other deep learning models that explicitly address global structure of images in every layer, sparse HMAX addresses local to global structure gradually along the hierarchy by applying patch-based learning to the output of the previous layer. As a consequence, the learning method can be standard sparse coding (SSC) or independent component analysis (ICA), two techniques deeply rooted in neuroscience. What makes SSC and ICA applicable at higher levels is the introduction of linear higher-order statistical regularities by max pooling. After training, high-level units display sparse, invariant selectivity for particular individuals or for image categories like those observed in human inferior temporal cortex (ITC) and medial temporal lobe (MTL). Finally, on an image classification benchmark, sparse HMAX outperforms the original HMAX by a large margin, suggesting its great potential for computer vision.},
author = {Hu, Xiaolin and Zhang, Jianwei and Li, Jianmin and Zhang, Bo},
doi = {10.1371/journal.pone.0081813},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/SparseHMAX.pdf:pdf},
issn = {1932-6203},
journal = {PloS one},
month = jan,
number = {1},
pages = {e81813},
pmid = {24392078},
title = {{Sparsity-regularized HMAX for visual recognition.}},
volume = {9},
year = {2014}
}

@inproceedings{Nair2013,
author = {Nair, Prashant and Chou, Chia-Chen and Qureshi, Moinuddin K.},
booktitle = {IEEE 19th International Symposium on High Performance Computer Architecture (HPCA)},
doi = {10.1109/HPCA.2013.6522355},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Refresh\_Pausing.pdf:pdf},
isbn = {978-1-4673-5587-2},
keywords = {refresh},
mendeley-tags = {refresh},
month = feb,
pages = {627--638},
publisher = {Ieee},
title = {{A case for Refresh Pausing in DRAM memory systems}},
year = {2013}
}

@article{Kunar2007,
abstract = {Contextual cuing experiments show that when displays are repeated, reaction times to find a target decrease over time even when observers are not aware of the repetition. It has been thought that the context of the display guides attention to the target. The authors tested this hypothesis by comparing the effects of guidance in a standard search task with the effects of contextual cuing. First, in standard search, an improvement in guidance causes search slopes (derived from Reaction Time x Set Size functions) to decrease. In contrast, the authors found that search slopes in contextual cuing did not become more efficient over time (Experiment 1). Second, when guidance was optimal (e.g., in easy feature search), they still found a small but reliable contextual cuing effect (Experiments 2a and 2b), suggesting that other factors, such as response selection, contribute to the effect. Experiment 3 supported this hypothesis by showing that the contextual cuing effect disappeared when the authors added interference to the response selection process. Overall, the data suggest that the relationship between guidance and contextual cuing is weak and that response selection can account for part of the effect.},
author = {Kunar, Melina a and Flusberg, Stephen and Horowitz, Todd S and Wolfe, Jeremy M},
doi = {10.1037/0096-1523.33.4.816},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Kunar et al.\_Does contextual cuing guide the deployment of attention\_2007.pdf:pdf},
issn = {0096-1523},
journal = {Journal of experimental psychology. Human perception and performance},
keywords = {Adolescent,Adult,Attention,Cues,Female,Humans,Male,Middle Aged,Reaction Time,Visual Perception},
month = aug,
number = {4},
pages = {816--28},
pmid = {17683230},
title = {{Does contextual cuing guide the deployment of attention?}},
volume = {33},
year = {2007}
}

@article{Crouzet2011,
abstract = {Research progress in machine vision has been very significant in recent years. Robust face detection and identification algorithms are already readily available to consumers, and modern computer vision algorithms for generic object recognition are now coping with the richness and complexity of natural visual scenes. Unlike early vision models of object recognition that emphasized the role of figure-ground segmentation and spatial information between parts, recent successful approaches are based on the computation of loose collections of image features without prior segmentation or any explicit encoding of spatial relations. While these models remain simplistic models of visual processing, they suggest that, in principle, bottom-up activation of a loose collection of image features could support the rapid recognition of natural object categories and provide an initial coarse visual representation before more complex visual routines and attentional mechanisms take place. Focusing on biologically plausible computational models of (bottom-up) pre-attentive visual recognition, we review some of the key visual features that have been described in the literature. We discuss the consistency of these feature-based representations with classical theories from visual psychology and test their ability to account for human performance on a rapid object categorization task.},
author = {Crouzet, S\'{e}bastien M and Serre, Thomas},
doi = {10.3389/fpsyg.2011.00326},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Serre\_What\_are\_the\_features\_underlying\_rapid\_object\_recognition.pdf:pdf},
issn = {1664-1078},
journal = {Frontiers in Psychology},
keywords = {computational models,computer vision,feedforward,rapid visual object recognition,visual features},
month = jan,
number = {Nov},
pmid = {22110461},
title = {{What are the Visual Features Underlying Rapid Object Recognition?}},
volume = {2},
year = {2011}
}

@inproceedings{CarrollAaronHeiser2010,
author = {{Carroll, Aaron Heiser}, Gernot},
booktitle = {Usenix Annual Technical Conference (ATC)},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Carroll.pdf:pdf},
title = {{An Analysis of Power Consumption in a Smartphone}},
year = {2010}
}

@article{Rutishauser,
author = {Rutishauser, U. and Walther, D. and Koch, C. and Perona, P.},
doi = {10.1109/CVPR.2004.1315142},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Rutishauser et al.\_Is bottom-up attention useful for object recognition\_Unknown.pdf:pdf},
isbn = {0-7695-2158-4},
journal = {Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.},
pages = {37--44},
publisher = {Ieee},
title = {{Is bottom-up attention useful for object recognition?}},
volume = {2}
}

@inproceedings{Fergus2004,
author = {Fergus, R. and Perona, P.},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition Workshop on Generative-Model Based Vision},
doi = {10.1109/CVPR.2004.383},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Learning generative visual models from few training examples- an incremental Bayesian approach tested on 101 object categories.pdf:pdf},
keywords = {Object},
mendeley-tags = {Object},
pages = {178--178},
publisher = {Ieee},
title = {{Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories}},
year = {2004}
}

@inproceedings{Lee2013,
author = {Lee, Donghyuk and Kim, Yoongu and Seshadri, Vivek and Liu, Jamie and Subramanian, Lavanya and Mutlu, Onur},
booktitle = {2013 IEEE 19th International Symposium on High Performance Computer Architecture (HPCA)},
doi = {10.1109/HPCA.2013.6522354},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Tiered-Latency DRAM- A Low Latency and Low Cost DRAM Architecture.pdf:pdf},
isbn = {978-1-4673-5587-2},
month = feb,
pages = {615--626},
publisher = {Ieee},
title = {{Tiered-latency DRAM: A low latency and low cost DRAM architecture}},
year = {2013}
}

@inproceedings{Kestur2012,
author = {Kestur, Srinidhi and Park, Mi Sun and Sabarad, Jagdish and Dantara, Dharav and Narayanan, Vijaykrishnan and Chen, Yang and Khosla, Deepak},
booktitle = {IEEE International Symposium on Field-Programmable Custom Computing Machines},
doi = {10.1109/FCCM.2012.33},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/kestur\_neo2\_fccm2012.pdf:pdf},
isbn = {978-1-4673-1605-7},
keywords = {accelerator,fpga,hmax,neuromorphic,recognition,saliency,vision},
month = apr,
pages = {141--148},
publisher = {Ieee},
title = {{Emulating Mammalian Vision on Reconfigurable Hardware}},
year = {2012}
}

@article{Schomberg1995,
abstract = {The authors explore a computational method for reconstructing an n-dimensional signal f from a sampled version of its Fourier transform f;. The method involves a window function w; and proceeds in three steps. First, the convolution g;=w;*f; is computed numerically on a Cartesian grid, using the available samples of f;. Then, g=wf is computed via the inverse discrete Fourier transform, and finally f is obtained as g/w. Due to the smoothing effect of the convolution, evaluating w;*f; is much less error prone than merely interpolating f;. The method was originally devised for image reconstruction in radio astronomy, but is actually applicable to a broad range of reconstructive imaging methods, including magnetic resonance imaging and computed tomography. In particular, it provides a fast and accurate alternative to the filtered backprojection. The basic method has several variants with other applications, such as the equidistant resampling of arbitrarily sampled signals or the fast computation of the Radon (Hough) transform.},
author = {Schomberg, H and Timmer, J},
doi = {10.1109/42.414625},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Schomberg, Timmer\_The gridding method for image reconstruction by Fourier transformation.\_1995.pdf:pdf},
issn = {0278-0062},
journal = {IEEE transactions on medical imaging},
month = jan,
number = {3},
pages = {596--607},
pmid = {18215864},
title = {{The gridding method for image reconstruction by Fourier transformation.}},
volume = {14},
year = {1995}
}

@article{Rajashekar2008,
abstract = {The ability to automatically detect visually interesting regions in images has many practical applications, especially in the design of active machine vision and automatic visual surveillance systems. Analysis of the statistics of image features at observers' gaze can provide insights into the mechanisms of fixation selection in humans. Using a foveated analysis framework, we studied the statistics of four low-level local image features: luminance, contrast, and bandpass outputs of both luminance and contrast, and discovered that image patches around human fixations had, on average, higher values of each of these features than image patches selected at random. Contrast-bandpass showed the greatest difference between human and random fixations, followed by luminance-bandpass, RMS contrast, and luminance. Using these measurements, we present a new algorithm that selects image regions as likely candidates for fixation. These regions are shown to correlate well with fixations recorded from human observers.},
author = {Rajashekar, U and van der Linde, I and Bovik, a C and Cormack, L K},
doi = {10.1109/TIP.2008.917218},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Rajashekar et al.\_GAFFE a gaze-attentive fixation finding engine.\_2008.pdf:pdf},
issn = {1057-7149},
journal = {IEEE Transactions on Image Processing},
keywords = {Algorithms,Artificial Intelligence,Attention,Attention: physiology,Automated,Automated: methods,Biological,Biomimetics,Biomimetics: methods,Computer Simulation,Computer-Assisted,Computer-Assisted: methods,Fixation,Humans,Image Enhancement,Image Enhancement: methods,Image Interpretation,Models,Ocular,Ocular: physiology,Pattern Recognition,Reproducibility of Results,Sensitivity and Specificity,Visual,Visual: physiology},
month = apr,
number = {4},
pages = {564--73},
pmid = {18390364},
title = {{GAFFE: A Gaze-Attentive Fixation Finding Engine}},
volume = {17},
year = {2008}
}

@article{Gupta2007,
author = {Gupta, Abhinav and Davis, Larry S and Park, College},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/cvpr\_2007.pdf:pdf},
journal = {IEEE Conference on Computer Vision and Pattern Recognition},
keywords = {scene},
mendeley-tags = {scene},
number = {2},
title = {{Objects in Action : An Approach for Combining Action Understanding and Object Perception}},
year = {2007}
}

@article{Advani2013,
author = {Advani, Siddharth and Sustersic, John and Irick, Kevin and Narayanan, Vijaykrishnan},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Advani\_ICASSP\_2013\_Paper.pdf:pdf},
journal = {IEEE Proceedings of The 38th International Conference on Acoustics, Speech, and Signal Processing},
keywords = {Foveation,Saliency},
mendeley-tags = {Foveation,Saliency},
title = {{A Multi-Resolution Saliency Framework To Drive Foveation}},
year = {2013}
}

@article{Azzopardi2012,
abstract = {Simple cells in primary visual cortex are believed to extract local contour information from a visual scene. The 2D Gabor function (GF) model has gained particular popularity as a computational model of a simple cell. However, it short-cuts the LGN, it cannot reproduce a number of properties of real simple cells, and its effectiveness in contour detection tasks has never been compared with the effectiveness of alternative models. We propose a computational model that uses as afferent inputs the responses of model LGN cells with center-surround receptive fields (RFs) and we refer to it as a Combination of Receptive Fields (CORF) model. We use shifted gratings as test stimuli and simulated reverse correlation to explore the nature of the proposed model. We study its behavior regarding the effect of contrast on its response and orientation bandwidth as well as the effect of an orthogonal mask on the response to an optimally oriented stimulus. We also evaluate and compare the performances of the CORF and GF models regarding contour detection, using two public data sets of images of natural scenes with associated contour ground truths. The RF map of the proposed CORF model, determined with simulated reverse correlation, can be divided in elongated excitatory and inhibitory regions typical of simple cells. The modulated response to shifted gratings that this model shows is also characteristic of a simple cell. Furthermore, the CORF model exhibits cross orientation suppression, contrast invariant orientation tuning and response saturation. These properties are observed in real simple cells, but are not possessed by the GF model. The proposed CORF model outperforms the GF model in contour detection with high statistical confidence (RuG data set: p<10(-4), and Berkeley data set: p<10(-4)). The proposed CORF model is more realistic than the GF model and is more effective in contour detection, which is assumed to be the primary biological role of simple cells.},
author = {Azzopardi, George and Petkov, Nicolai},
doi = {10.1007/s00422-012-0486-6},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/A CORF Computational Model of a simple cell that relies on LGN input outperforms the Gabor function model.pdf:pdf},
issn = {1432-0770},
journal = {Biological Cybernetics},
keywords = {Models,Neurons,Neurons: cytology,Theoretical,Visual Cortex,Visual Cortex: cytology},
month = mar,
number = {3},
pages = {177--89},
pmid = {22526357},
title = {{A CORF Computational Model of a Simple Cell that Relies on LGN Input Outperforms the Gabor Function Model}},
volume = {106},
year = {2012}
}

@article{Torralba2003,
abstract = {In this paper we study the statistical properties of natural images belonging to different categories and their relevance for scene and object categorization tasks. We discuss how second-order statistics are correlated with image categories, scene scale and objects. We propose how scene categorization could be computed in a feedforward manner in order to provide top-down and contextual information very early in the visual processing chain. Results show how visual categorization based directly on low-level features, without grouping or segmentation stages, can benefit object localization and identification. We show how simple image statistics can be used to predict the presence and absence of objects in the scene before exploring the image.},
author = {Torralba, Antonio and Oliva, Aude},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Torralba, Oliva\_Statistics of natural image categories.\_2003.pdf:pdf},
issn = {0954-898X},
journal = {Network (Bristol, England)},
keywords = {Nature,Photic Stimulation,Photic Stimulation: methods,Statistics as Topic},
month = aug,
number = {3},
pages = {391--412},
pmid = {12938764},
title = {{Statistics of natural image categories.}},
volume = {14},
year = {2003}
}

@article{Muller2006,
author = {M\"{u}ller, Hermann J. and Krummenacher, Joseph},
doi = {10.1080/13506280500527676},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/M\"{u}ller, Krummenacher\_Visual search and selective attention\_2006.pdf:pdf},
isbn = {1350628050},
issn = {1350-6285},
journal = {Visual Cognition},
month = aug,
number = {4-8},
pages = {389--410},
title = {{Visual search and selective attention}},
volume = {14},
year = {2006}
}

@article{Riesenhuber1999,
abstract = {Visual processing in cortex is classically modeled as a hierarchy of increasingly sophisticated representations, naturally extending the model of simple to complex cells of Hubel and Wiesel. Surprisingly, little quantitative modeling has been done to explore the biological feasibility of this class of models to explain aspects of higher-level visual processing such as object recognition. We describe a new hierarchical model consistent with physiological data from inferotemporal cortex that accounts for this complex visual task and makes testable predictions. The model is based on a MAX-like operation applied to inputs to certain cortical neurons that may have a general role in cortical function.},
author = {Riesenhuber, M and Poggio, T},
doi = {10.1038/14819},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Poggio\_Hierarchical\_models\_of\_object\_recognition\_in\_cortex.pdf:pdf},
issn = {1097-6256},
journal = {Nature Neuroscience},
keywords = {Animals,Computer Simulation,Form Perception,Form Perception: physiology,Hierarchy,Macaca,Mental Recall,Mental Recall: physiology,Models,Neurological,Neurons,Neurons: physiology,Object,Visual Cortex,Visual Cortex: cytology,Visual Cortex: physiology,Visual Fields,Visual Fields: physiology},
mendeley-tags = {Hierarchy,Object},
month = nov,
number = {11},
pages = {1019--25},
pmid = {10526343},
title = {{Hierarchical models of object recognition in cortex.}},
volume = {2},
year = {1999}
}

@article{Kunar2006,
abstract = {In visual search tasks, attention can be guided to a target item--appearing amidst distractors--on the basis of simple features (e.g., finding the red letter among green). Chun and Jiang's (1998) contextual cuing effect shows that reaction times (RTs) are also speeded if the spatial configuration of items in a scene is repeated over time. In the present studies, we ask whether global properties of the scene can speed search (e.g., if the display is mostly red, then the target is at location X). In Experiment 1A, the overall background color of the display predicted the target location, and the predictive color could appear 0, 400, or 800 msec in advance of the search array. Mean RTs were faster in predictive than in nonpredictive conditions. However, there was little improvement in search slopes. The global color cue did not improve search efficiency. Experiments 1B-1F replicated this effect using different predictive properties (e.g., background orientation-texture and stimulus color). The results showed a strong RT effect of predictive background, but (at best) only a weak improvement in search efficiency. A strong improvement in efficiency was found, however, when the informative background was presented 1,500 msec prior to the onset of the search stimuli and when observers were given explicit instructions to use the cue (Experiment 2).},
author = {Kunar, Melina a and Flusberg, Stephen J and Wolfe, Jeremy M},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Kunar, Flusberg, Wolfe\_Contextual cuing by global features.\_2006.pdf:pdf},
issn = {0031-5117},
journal = {Perception \& psychophysics},
keywords = {Adolescent,Adult,Attention,Color Perception,Cues,Female,Field Dependence-Independence,Humans,Judgment,Male,Middle Aged,Orientation,Reaction Time},
month = oct,
number = {7},
pages = {1204--16},
pmid = {17355043},
title = {{Contextual cuing by global features.}},
volume = {68},
year = {2006}
}

@article{Goodrich2012,
author = {Goodrich, Ben and Arel, Itamar},
doi = {10.1109/CVPRW.2012.6239177},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Goodrich, Arel\_Reinforcement learning based visual attention with application to face detection\_2012.pdf:pdf},
isbn = {978-1-4673-1612-5},
journal = {2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
month = jun,
pages = {19--24},
publisher = {Ieee},
title = {{Reinforcement learning based visual attention with application to face detection}},
year = {2012}
}

@article{Bae2011,
author = {Bae, Sungmin and Cho, Yong Cheol Peter and Park, Sungho and Irick, Kevin M. and Jin, Yongseok and Narayanan, Vijaykrishnan},
doi = {10.1109/FCCM.2011.41},
file = {:Users/siddharthadvani/Documents/MDL/ARL/Papers/MDL\_An FPGA implementation of Information Theoretic Visual-Saliency System and Its Optimization.pdf:pdf},
isbn = {978-1-61284-277-6},
journal = {2011 IEEE 19th Annual International Symposium on Field-Programmable Custom Computing Machines},
keywords = {FPGA,Saliency},
mendeley-tags = {FPGA,Saliency},
month = may,
pages = {41--48},
publisher = {Ieee},
title = {{An FPGA Implementation of Information Theoretic Visual-Saliency System and Its Optimization}},
year = {2011}
}

@article{Traver2008,
author = {Traver, V. Javier and Pla, Filiberto},
doi = {10.1016/j.imavis.2007.11.009},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Traver, Pla\_Log-polar mapping template design From task-level requirements to geometry parameters\_2008.pdf:pdf},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {design criteria,genetic algorithm,log-polar vision,receptive fields},
month = oct,
number = {10},
pages = {1354--1370},
title = {{Log-polar mapping template design: From task-level requirements to geometry parameters}},
volume = {26},
year = {2008}
}

@article{Ghodrati2012,
abstract = {Humans can effectively and swiftly recognize objects in complex natural scenes. This outstanding ability has motivated many computational object recognition models. Most of these models try to emulate the behavior of this remarkable system. The human visual system hierarchically recognizes objects in several processing stages. Along these stages a set of features with increasing complexity is extracted by different parts of visual system. Elementary features like bars and edges are processed in earlier levels of visual pathway and as far as one goes upper in this pathway more complex features will be spotted. It is an important interrogation in the field of visual processing to see which features of an object are selected and represented by the visual cortex. To address this issue, we extended a hierarchical model, which is motivated by biology, for different object recognition tasks. In this model, a set of object parts, named patches, extracted in the intermediate stages. These object parts are used for training procedure in the model and have an important role in object recognition. These patches are selected indiscriminately from different positions of an image and this can lead to the extraction of non-discriminating patches which eventually may reduce the performance. In the proposed model we used an evolutionary algorithm approach to select a set of informative patches. Our reported results indicate that these patches are more informative than usual random patches. We demonstrate the strength of the proposed model on a range of object recognition tasks. The proposed model outperforms the original model in diverse object recognition tasks. It can be seen from the experiments that selected features are generally particular parts of target images. Our results suggest that selected features which are parts of target objects provide an efficient set for robust object recognition.},
author = {Ghodrati, Masoud and Khaligh-Razavi, Seyed-Mahdi and Ebrahimpour, Reza and Rajaei, Karim and Pooyan, Mohammad},
doi = {10.1371/journal.pone.0032357},
file = {:Users/siddharthadvani/Documents/MDL/ARL/Papers/Ghodrati\_BioinspiredFeatures.pdf:pdf},
issn = {1932-6203},
journal = {PloS one},
keywords = {Algorithms,Brain Mapping,Brain Mapping: methods,Computational Biology,Computational Biology: methods,Computer Simulation,Humans,Models, Statistical,Models, Theoretical,Normal Distribution,Pattern Recognition, Visual,Photic Stimulation,Photic Stimulation: methods,Recognition (Psychology),Vision, Ocular,Visual Pathways,Visual Perception},
month = jan,
number = {2},
pages = {e32357},
pmid = {22384229},
title = {{How can selection of biologically inspired features improve the performance of a robust object recognition model?}},
volume = {7},
year = {2012}
}

@article{Schrauwen,
abstract = {Hardware implementations of Spiking Neural Networks are numerous because they are well suited for implementation in digital and analog hardware, and outperform classic neural networks. This work presents an application driven digital hardware exploration where we implement real-time, isolated digit speech recognition using a Liquid State Machine. The Liquid State Machine is a recurrent neural network of spiking neurons where only the output layer is trained. First we test two existing hardware architectures which we improve and extend, but that appears to be too fast and thus area consuming for this application. Next, we present a scalable, serialized architecture that allows a very compact implementation of spiking neural networks that is still fast enough for real-time processing. All architectures support leaky integrate-and-fire membranes with exponential synaptic models. This work shows that there is actually a large hardware design space of Spiking Neural Network hardware that can be explored. Existing architectures have only spanned part of it.},
author = {Schrauwen, Benjamin and D'Haene, Michiel and Verstraeten, David and Campenhout, Jan Van},
doi = {10.1016/j.neunet.2007.12.009},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Schrauwen et al.\_Compact hardware liquid state machines on FPGA for real-time speech recognition.\_Unknown.pdf:pdf},
issn = {0893-6080},
journal = {Neural networks : the official journal of the International Neural Network Society},
keywords = {Action Potentials,Analog-Digital Conversion,Humans,Models, Neurological,Neural Networks (Computer),Recognition (Psychology),Signal Processing, Computer-Assisted,Speech,Time Factors},
number = {2-3},
pages = {511--23},
pmid = {18222634},
title = {{Compact hardware liquid state machines on FPGA for real-time speech recognition.}},
volume = {21}
}

@inproceedings{Chen2014,
author = {Chen, Tianshi and Wang, Jia and Chen, Yunji and Temam, Olivier},
booktitle = {International Conference on Architectural Support for Programming Languages and Operating Systems},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/DianNao.pdf:pdf},
isbn = {9781450323055},
keywords = {Accelerator},
mendeley-tags = {Accelerator},
title = {{DianNao : A Small-Footprint High-Throughput Accelerator for Ubiquitous Machine-Learning}},
year = {2014}
}

@phdthesis{Maashri2012,
author = {Maashri, Ahmed Al},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/maashri\_accelerated\_embedded\_vision\_systems\_v4.pdf:pdf},
number = {December},
title = {{Accelerating design and implementation of embedded vision systems}},
year = {2012}
}

@phdthesis{Dantara2011,
author = {Dantara, Dharav},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Dharav\_Dantara\_MS\_Thesis.pdf:pdf},
keywords = {Accelerator},
mendeley-tags = {Accelerator},
number = {August},
school = {The Pennsylvania State University},
title = {{Reconfigurable Accelerators for Neuromorphic Systems}},
type = {MS Thesis},
year = {2011}
}

@article{Shu-Ying2009,
abstract = {This paper presents an integrated method by using optical flow and kernel particle filter (KPF) to detect and track moving targets in omnidirectional vision. According to the circle character in omnidirectional image, the algorithms of optical flow fields and kernel particle filter are improved based on the polar coordinates at the omnidirectional center. The edge of a motion object can be detected by optical flow fields and is surrounded by a reference region. In order to resolve some shape distortions such as rotation and scaling in the omnidirectional image a dynamic elliptical template with affine transformations is constructed and its motion model is established to predict particle state. Histograms are used as the features in the reference region and particle regions. The Bhattacharyya distance is computed for particle weights. Gaussian kernel function is used in kernel particle filter. Experiment results show that the method can detect and track moving objects and has better performance at real-time and accuracy.},
author = {Shu-Ying, Yang and WeiMin, Ge and Cheng, Zhang},
doi = {10.1016/j.visres.2008.11.002},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Shu-Ying, WeiMin, Cheng\_Tracking unknown moving targets on omnidirectional vision.\_2009.pdf:pdf},
issn = {1878-5646},
journal = {Vision research},
keywords = {Algorithms,Artificial Intelligence,Computer Simulation,Models, Theoretical,Motion,Pattern Recognition, Automated,Pattern Recognition, Automated: methods},
month = feb,
number = {3},
pages = {362--7},
pmid = {19032961},
publisher = {Elsevier Ltd},
title = {{Tracking unknown moving targets on omnidirectional vision.}},
volume = {49},
year = {2009}
}

@article{Lleras2004,
abstract = {Marvin M. Chun and Yuhong Jiang (1998) investigated the role of spatial context on visual search. They used two display conditions. In the Old Display condition, the spatial arrangement of items in the search display was kept constant throughout the experiment. In the New Display condition, the spatial arrangement of items was always novel from trial to trial. The results showed better performance with Old Displays than with New Displays. The authors proposed that repeated spatial context help guiding attention to the target location, thus they termed this effect Contextual Cueing. We present three attempts to reproduce this effect. Experiments 1 and 2 were near exact replications of experiments in Chun and Jiang's report, where we failed to obtain Contextual Cueing. Post-experimental interviews revealed that participants used different search strategies when performing the task: an 'active' strategy (an active effort to find the target), or a 'passive' strategy (intuitive search). In Experiment 3, we manipulated task instructions to bias participants into using active or passive strategies. A robust Contextual Cueing Effect was obtained only in the passive instruction condition.},
author = {Lleras, Alejandro and {Von M\"{u}hlenen}, Adrian},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Lleras, Von M\"{u}hlenen\_Spatial context and top-down strategies in visual search.\_2004.pdf:pdf},
issn = {0169-1015},
journal = {Spatial vision},
keywords = {Adult,Attention,Cues,Female,Humans,Individuality,Male,Random Allocation,Reaction Time,Space Perception},
month = jan,
number = {4-5},
pages = {465--82},
pmid = {15559114},
title = {{Spatial context and top-down strategies in visual search.}},
volume = {17},
year = {2004}
}

@article{Bruceb,
author = {Bruce, Neil D B},
keywords = {Saliency},
mendeley-tags = {Saliency},
title = {{An Information Theoretic Model of Saliency and Visual Search}},
}

@article{Belongie2002,
author = {Belongie, S. and Malik, J. and Puzicha, J.},
doi = {10.1109/34.993558},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Belongie, Malik, Puzicha\_Shape matching and object recognition using shape contexts\_2002.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = apr,
number = {4},
pages = {509--522},
title = {{Shape matching and object recognition using shape contexts}},
volume = {5395},
year = {2009}
}

@misc{GodwinDwayne2012,
author = {{Godwin Dwayne}, Cham Jorge},
booktitle = {Scientific American},
keywords = {brain},
mendeley-tags = {brain},
title = {{Your Brain by the Numbers}},
year = {2012}
}

@phdthesis{Kestur2012a,
author = {Kestur, Srinidhi},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/1-SrinidhiKestur-Dissertation.pdf:pdf},
number = {May},
title = {{Domain-Specific Accelerators on Reconfigurable Platforms}},
year = {2012}
}

@inproceedings{Maashri2012a,
address = {New York, New York, USA},
author = {Maashri, Ahmed Al and Debole, Michael and Cotter, Matthew and Chandramoorthy, Nandhini and Xiao, Yang and Narayanan, Vijaykrishnan and Chakrabarti, Chaitali},
booktitle = {Design Automation Conference},
doi = {10.1145/2228360.2228465},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/MDL\_neuromorphic\_dac12.pdf:pdf},
isbn = {9781450311991},
keywords = {Object,domain-specific,power efficiency,recognition,system},
mendeley-tags = {Object},
pages = {579},
publisher = {ACM Press},
title = {{Accelerating neuromorphic vision algorithms for recognition}},
year = {2012}
}

@inproceedings{Clemons2012,
author = {Clemons, Jason and Zhu, Haishan and Savarese, Silvio and Austin, Todd and Arbor, Ann},
booktitle = {IEEE International Symposium on Workload Characterization},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/MEVBench.pdf:pdf},
title = {{MEVBench : A Mobile Computer Vision Benchmarking Suite}},
year = {2012}
}

@article{Bruce2009a,
abstract = {A proposal for saliency computation within the visual cortex is put forth based on the premise that localized saliency computation serves to maximize information sampled from one's environment. The model is built entirely on computational constraints but nevertheless results in an architecture with cells and connectivity reminiscent of that appearing in the visual cortex. It is demonstrated that a variety of visual search behaviors appear as emergent properties of the model and therefore basic principles of coding and information transmission. Experimental results demonstrate greater efficacy in predicting fixation patterns across two different data sets as compared with competing models.},
author = {Bruce, Neil D B and Tsotsos, John K},
doi = {10.1167/9.3.5},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Bruce, Tsotsos\_Saliency, attention, and visual search an information theoretic approach.\_2009.pdf:pdf},
issn = {1534-7362},
journal = {Journal of Vision},
keywords = {Attention,Attention: physiology,Eye Movements,Eye Movements: physiology,Fixation,Humans,Information Theory,Models,Neurological,Ocular,Ocular: physiology,Pattern Recognition,Photic Stimulation,Saliency,Visual,Visual Cortex,Visual Cortex: physiology,Visual: physiology},
language = {en},
mendeley-tags = {Saliency},
month = jan,
number = {3},
pages = {5.1--24},
pmid = {19757944},
publisher = {Association for Research in Vision and Ophthalmology},
title = {{Saliency, Attention, and Visual Search: An Information Theoretic Approach}},
volume = {9},
year = {2009}
}

@article{Yamamoto1996,
author = {Yamamoto, Hiroyuki and Yeshurun, Yehezkel and Levine, Martin D.},
doi = {10.1006/cviu.1996.0004},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Yamamoto, Yeshurun, Levine\_An Active Foveated Vision System Attentional Mechanisms and Scan Path Covergence Measures\_1996.pdf:pdf},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
month = jan,
number = {1},
pages = {50--65},
title = {{An Active Foveated Vision System: Attentional Mechanisms and Scan Path Covergence Measures}},
volume = {63},
year = {1996}
}

@book{Gonzalez2007,
author = {Gonzalez, Rafael C. and Woods, Richard E.},
isbn = {013168728X},
keywords = {Image Processing},
mendeley-tags = {Image Processing},
publisher = {Prentice Hall},
title = {{Digital Image Processing (3rd Edition)}},
year = {2007}
}

@inproceedings{Mukundan2013,
author = {Mukundan, Janani and Hunter, Hillery and Kim, Kyu-hyoun and Stuecheli, Jeffrey},
booktitle = {International Symposium on Computer Architecture},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/isca13-mukundan.pdf:pdf},
isbn = {9781450320795},
keywords = {refresh},
mendeley-tags = {refresh},
title = {{Understanding and Mitigating Refresh Overheads in High-Density DDR4 DRAM Systems}},
year = {2013}
}

@inproceedings{Stuecheli2010,
author = {Stuecheli, Jeffrey and Kaseridis, Dimitris and C.Hunter, Hillery and John, Lizy K.},
booktitle = {43rd Annual IEEE/ACM International Symposium on Microarchitecture},
doi = {10.1109/MICRO.2010.22},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Elastic Refresh.pdf:pdf},
isbn = {978-1-4244-9071-4},
month = dec,
pages = {375--384},
publisher = {Ieee},
title = {{Elastic Refresh: Techniques to Mitigate Refresh Penalties in High Density Memory}},
year = {2010}
}

@article{Peters2007,
author = {Peters, Robert J and Itti, Laurent},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Peters\_Itti08tap.pdf:pdf},
journal = {ACM Transactions on Applied Perception},
number = {May},
pages = {1--21},
title = {{Applying computational tools to predict gaze direction in interactive visual environments}},
volume = {i},
year = {2007}
}

@article{Schrauwen2008,
abstract = {Hardware implementations of Spiking Neural Networks are numerous because they are well suited for implementation in digital and analog hardware, and outperform classic neural networks. This work presents an application driven digital hardware exploration where we implement real-time, isolated digit speech recognition using a Liquid State Machine. The Liquid State Machine is a recurrent neural network of spiking neurons where only the output layer is trained. First we test two existing hardware architectures which we improve and extend, but that appears to be too fast and thus area consuming for this application. Next, we present a scalable, serialized architecture that allows a very compact implementation of spiking neural networks that is still fast enough for real-time processing. All architectures support leaky integrate-and-fire membranes with exponential synaptic models. This work shows that there is actually a large hardware design space of Spiking Neural Network hardware that can be explored. Existing architectures have only spanned part of it.},
author = {Schrauwen, Benjamin and D'Haene, Michiel and Verstraeten, David and Campenhout, Jan Van},
doi = {10.1016/j.neunet.2007.12.009},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Schrauwen et al.\_Compact hardware liquid state machines on FPGA for real-time speech recognition.\_2008.pdf:pdf},
issn = {0893-6080},
journal = {Neural networks : the official journal of the International Neural Network Society},
keywords = {Action Potentials,Analog-Digital Conversion,Humans,Models, Neurological,Neural Networks (Computer),Recognition (Psychology),Signal Processing, Computer-Assisted,Speech,Time Factors},
number = {2-3},
pages = {511--23},
pmid = {18222634},
title = {{Compact hardware liquid state machines on FPGA for real-time speech recognition.}},
volume = {21},
year = {2008}
}

@article{Irick2009,
author = {Irick, K. M. and DeBole, M. and Park, S. and {Al Maashri}, a. and Kestur, S. and Yu, C.-L. and Vijaykrishnan, N.},
doi = {10.1117/12.834177},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Irick et al.\_A scalable multi-FPGA framework for real-time digital signal processing\_2009.pdf:pdf},
journal = {Proceedings of SPIE},
keywords = {fpga design,image processing},
pages = {744416--744416--6},
publisher = {Spie},
title = {{A scalable multi-FPGA framework for real-time digital signal processing}},
volume = {7444},
year = {2009}
}

@article{Guo2010,
abstract = {Salient areas in natural scenes are generally regarded as areas which the human eye will typically focus on, and finding these areas is the key step in object detection. In computer vision, many models have been proposed to simulate the behavior of eyes such as SaliencyToolBox (STB), Neuromorphic Vision Toolkit (NVT), and others, but they demand high computational cost and computing useful results mostly relies on their choice of parameters. Although some region-based approaches were proposed to reduce the computational complexity of feature maps, these approaches still were not able to work in real time. Recently, a simple and fast approach called spectral residual (SR) was proposed, which uses the SR of the amplitude spectrum to calculate the image's saliency map. However, in our previous work, we pointed out that it is the phase spectrum, not the amplitude spectrum, of an image's Fourier transform that is key to calculating the location of salient areas, and proposed the phase spectrum of Fourier transform (PFT) model. In this paper, we present a quaternion representation of an image which is composed of intensity, color, and motion features. Based on the principle of PFT, a novel multiresolution spatiotemporal saliency detection model called phase spectrum of quaternion Fourier transform (PQFT) is proposed in this paper to calculate the spatiotemporal saliency map of an image by its quaternion representation. Distinct from other models, the added motion dimension allows the phase spectrum to represent spatiotemporal saliency in order to perform attention selection not only for images but also for videos. In addition, the PQFT model can compute the saliency map of an image under various resolutions from coarse to fine. Therefore, the hierarchical selectivity (HS) framework based on the PQFT model is introduced here to construct the tree structure representation of an image. With the help of HS, a model called multiresolution wavelet domain foveation (MWDF) is proposed in this paper to improve coding efficiency in image and video compression. Extensive tests of videos, natural images, and psychological patterns show that the proposed PQFT model is more effective in saliency detection and can predict eye fixations better than other state-of-the-art models in previous literature. Moreover, our model requires low computational cost and, therefore, can work in real time. Additional experiments on image and video compression show that the HS-MWDF model can achieve higher compression rate than the traditional model.},
author = {Guo, Chenlei and Zhang, Liming},
doi = {10.1109/TIP.2009.2030969},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/Guo, Zhang\_A novel multiresolution spatiotemporal saliency detection model and its applications in image and video compression.\_2010.pdf:pdf},
issn = {1941-0042},
journal = {IEEE Transactions on Image Processing},
keywords = {Saliency},
mendeley-tags = {Saliency},
month = jan,
number = {1},
pages = {185--98},
pmid = {19709976},
title = {{A Novel Multiresolution Spatiotemporal Saliency Detection Model and its Applications in Image and Video Compression}},
volume = {19},
year = {2010}
}

@misc{TheMendeleySupportTeam2011,
abstract = {A quick introduction to Mendeley. Learn how Mendeley creates your personal digital library, how to organize and annotate documents, how to collaborate and share with colleagues, and how to generate citations and bibliographies.},
address = {London},
author = {{The Mendeley Support Team}},
booktitle = {Mendeley Desktop},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/The Mendeley Support Team\_Getting Started with Mendeley\_2011.pdf:pdf},
keywords = {Mendeley,how-to,user manual},
pages = {1--16},
publisher = {Mendeley Ltd.},
title = {{Getting Started with Mendeley}},
year = {2011}
}

@article{Koehler2014,
author = {Koehler, Kathryn and Eckstein, Miguel P},
doi = {10.1167/14.3.14.doi},
file = {:Users/siddharthadvani/Documents/Mendeley Desktop/What do saliency models predict.pdf:pdf},
journal = {Journal of Vision},
keywords = {Saliency},
mendeley-tags = {Saliency},
pages = {1--27},
title = {{What do saliency models predict ?}},
volume = {14},
year = {2014}
}

@misc{vivado,
      title  = "{Vivado Design Suite}",
      booktitle = "Xilinx",
      url = {http://www.xilinx.com/products/design-tools/vivado/},
      year   = "2014",
    }

@misc{jedec-sdram-standards,
      title  = "{JEDEC DDR3 and DDR4 SDRAM Standard}",
      booktitle = "JEDEC",
      url = {http://www.jedec.org/category/technology-focus-area/main-memory-ddr3-ddr4-sdram},
      year   = "2012",
    }


@article{Itti1998,
  author = {L. Itti and C. Koch and E. Niebur},
  title = {A Model of Saliency-Based Visual Attention for Rapid Scene Analysis},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {20},
  number = {11},
  pages = {1254-1259},
  month = {Nov},
  year = {1998},
  keywords = {Visual attention ; target detection ; saliency ; image understanding},
  abstract = {A trainable visual attention system, inspired by the behavior and the neuronal architecture of the early primate visual system, is presented. Multiscale image features are combined into a single topographical saliency map. A dynamical neural network then selects attended locations in order of decreasing saliency. The system breaks down the complex problem of scene understanding by rapidly selecting, in a computationally efficient manner, conspicuous locations to be analyzed in detail.},
  type = {mod;bu;cv},
  file = {http://iLab.usc.edu/publications/doc/Itti_etal98pami.pdf},
  if = {1998 impact factor: 1.417},
}

@INPROCEEDINGS{Effex, 
author={Clemons, J. and Jones, A. and Perricone, R. and Savarese, S. and Austin, T.}, 
booktitle={Design Automation Conference (DAC), 2011 48th ACM/EDAC/IEEE}, 
title={EFFEX: An embedded processor for computer vision based feature extraction}, 
year={2011}, 
month={June}, 
pages={1020-1025}, 
keywords={computer vision;feature extraction;memory architecture;mobile computing;multiprocessing systems;EFFEX;computer vision software pipeline algorithm;embedded heterogeneous multicore processor;feature extraction algorithms;memory architecture;mobile applications;mobile vision performance;Algorithm design and analysis;Computer vision;Feature extraction;Mobile communication;Multicore processing;Random access memory;EFFEX;Feature extraction;Heterogenous Architecture}, 
ISSN={0738-100x}}

@INPROCEEDINGS{Farabet, 
author={Farabet, C. and Martini, B. and Corda, B. and Akselrod, P. and Culurciello, E. and LeCun, Y.}, 
booktitle={Computer Vision and Pattern Recognition Workshops (CVPRW), 2011 IEEE Computer Society Conference on}, 
title={NeuFlow: A runtime reconfigurable dataflow processor for vision}, 
year={2011}, 
month={June}, 
pages={109-116}, 
keywords={computer vision;field programmable gate arrays;flow graphs;NeuFlow;Xilinx Virtex 6 FPGA platform;dataflow compiler;flow graph representations;laptop computer;luaFlow;machine code;runtime reconfigurable dataflow processor;scalable dataflow hardware architecture;Computer architecture;Convolvers;Feature extraction;Field programmable gate arrays;Hardware;Runtime;Tiles}, 
doi={10.1109/CVPRW.2011.5981829}, 
ISSN={2160-7508}}

@ARTICLE{DRAMSim2,
author={Rosenfeld, P. and Cooper-Balis, E. and Jacob, B.},
journal={Computer Architecture Letters}, title={DRAMSim2: A Cycle Accurate Memory System Simulator},
year={2011},
month={jan.-june },
volume={10},
number={1},
pages={16 -19},
keywords={DDR2/3 memory system model;DRAMSim2 simulation;DRAMSim2 timing;Verilog model;cycle accurate memory system simulator;trace-based simulation;visualization tool;DRAM chips;memory architecture;memory cards;},
doi={10.1109/L-CA.2011.4},
ISSN={1556-6056},}


@INPROCEEDINGS{islped98, 
author={Ohsawa, T. and Kai, K. and Murakami, K.}, 
booktitle={Low Power Electronics and Design, 1998. Proceedings. 1998 International Symposium on}, 
title={Optimizing the DRAM refresh count for merged DRAM/logic LSIs}, 
year={1998}, 
month={Aug}, 
pages={82-87}, 
keywords={DRAM chips;circuit optimisation;integrated circuit design;integrated logic circuits;large scale integration;low-power electronics;memory architecture;DRAM refresh architectures;DRAM refresh count optimisation;data retention time;merged DRAM/logic LSIs;power consumption},}

@INPROCEEDINGS{action-recognition, 
author={Jhuang, H. and Serre, T. and Wolf, L. and Poggio, T.}, 
booktitle={Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on}, 
title={A Biologically Inspired System for Action Recognition}, 
year={2007}, 
month={Oct}, 
pages={1-8}, 
keywords={image motion analysis;image sequences;object recognition;video signal processing;action recognition;biologically inspired system;hierarchical feedforward architectures;motion processing;motion-direction sensitive units;neurobiological model;object recognition;position-invariant spatio-temporal feature detectors;video sequences;Biological system modeling;Brain modeling;Computer vision;Motion analysis;Motion detection;Object recognition;Position sensitive particle detectors;Sensor arrays;Testing;Video sequences}, 
doi={10.1109/ICCV.2007.4408988}, 
ISSN={1550-5499},}
